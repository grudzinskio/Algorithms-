{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sorting Algorithm Benchmark Analysis\n",
    "\n",
    "**Author:** Computer Science Student\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This project implements and benchmarks insertion sort and selection sort algorithms to analyze their runtime performance. We test both algorithms under best, average, and worst case scenarios and compare the results with theoretical time complexity predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm Implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insertion Sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insertion_sort(lst):\n",
    "    \"\"\"Sorts a list in-place using insertion sort algorithm.\"\"\"\n",
    "    for i in range(1, len(lst)):\n",
    "        key = lst[i]\n",
    "        j = i - 1\n",
    "        \n",
    "        while j >= 0 and lst[j] > key:\n",
    "            lst[j + 1] = lst[j]\n",
    "            j -= 1\n",
    "        \n",
    "        lst[j + 1] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selection_sort(lst):\n",
    "    \"\"\"Sorts a list in-place using selection sort algorithm.\"\"\"\n",
    "    for i in range(len(lst)):\n",
    "        min_idx = i\n",
    "        for j in range(i + 1, len(lst)):\n",
    "            if lst[j] < lst[min_idx]:\n",
    "                min_idx = j\n",
    "        \n",
    "        lst[i], lst[min_idx] = lst[min_idx], lst[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original list: [64, 34, 25, 12, 22, 11, 90]\n",
      "Sorted list: [11, 12, 22, 25, 34, 64, 90]\n",
      "Empty list after sorting: []\n",
      "Single element list after sorting: [42]\n",
      "Already sorted list after sorting: [1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "# Test insertion sort\n",
    "test_list = [64, 34, 25, 12, 22, 11, 90]\n",
    "insertion_sort(test_list)\n",
    "print(f\"Insertion sort test: {test_list}\")\n",
    "\n",
    "# Test edge cases\n",
    "test_cases = [[], [42], [1, 2, 3, 4, 5]]\n",
    "for case in test_cases:\n",
    "    insertion_sort(case)\n",
    "    print(f\"Edge case result: {case}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selection sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original list: [64, 34, 25, 12, 22, 11, 90]\n",
      "Sorted list: [11, 12, 22, 25, 34, 64, 90]\n",
      "Empty list after sorting: []\n",
      "Single element list after sorting: [42]\n",
      "Already sorted list after sorting: [1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "# Test selection sort\n",
    "test_list2 = [64, 34, 25, 12, 22, 11, 90]\n",
    "selection_sort(test_list2)\n",
    "print(f\"Selection sort test: {test_list2}\")\n",
    "\n",
    "# Test edge cases\n",
    "test_cases2 = [[], [42], [1, 2, 3, 4, 5]]\n",
    "for case in test_cases2:\n",
    "    selection_sort(case)\n",
    "    print(f\"Edge case result: {case}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(sort_func, input_list):\n",
    "    \"\"\"Measures runtime of sorting function on given input.\"\"\"\n",
    "    # Create copy to avoid modifying original list\n",
    "    test_list = input_list.copy()\n",
    "    \n",
    "    # Measure elapsed time with high precision\n",
    "    start_time = time.perf_counter()\n",
    "    sort_func(test_list)\n",
    "    end_time = time.perf_counter()\n",
    "    \n",
    "    return end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test_data(size, case_type):\n",
    "    \"\"\"Generates test data for different scenarios.\"\"\"\n",
    "    if case_type == 'best':\n",
    "        # Best case: already sorted list\n",
    "        return list(range(size))\n",
    "    elif case_type == 'average':\n",
    "        # Average case: randomly shuffled list\n",
    "        data = list(range(size))\n",
    "        random.shuffle(data)\n",
    "        return data\n",
    "    elif case_type == 'worst':\n",
    "        # Worst case: reverse sorted list\n",
    "        return list(range(size, 0, -1))\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown case_type: {case_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting comprehensive benchmark data collection...\n",
      "List sizes to test: [1000, 2000, 5000, 10000, 20000]\n",
      "Cases to test: best (sorted), average (random), worst (reverse sorted)\n",
      "Algorithms to test: insertion_sort, selection_sort\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define list sizes spanning multiple orders of magnitude\n",
    "list_sizes = [1000, 2000, 5000, 10000, 20000]\n",
    "\n",
    "# Initialize results structure\n",
    "benchmark_results = {\n",
    "    'insertion_sort': {\n",
    "        'best': {'sizes': [], 'times': []},\n",
    "        'average': {'sizes': [], 'times': []},\n",
    "        'worst': {'sizes': [], 'times': []}\n",
    "    },\n",
    "    'selection_sort': {\n",
    "        'best': {'sizes': [], 'times': []},\n",
    "        'average': {'sizes': [], 'times': []},\n",
    "        'worst': {'sizes': [], 'times': []}\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Running benchmarks...\")\n",
    "print(f\"List sizes: {list_sizes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking Insertion Sort...\n",
      "  Testing size 1000:\n",
      "    Best case: 0.000107 seconds\n",
      "    Average case: 0.020392 seconds\n",
      "    Worst case: 0.037938 seconds\n",
      "\n",
      "  Testing size 2000:\n",
      "    Best case: 0.000157 seconds\n",
      "    Average case: 0.064330 seconds\n",
      "    Worst case: 0.164764 seconds\n",
      "\n",
      "  Testing size 5000:\n",
      "    Best case: 0.000918 seconds\n",
      "    Average case: 0.490143 seconds\n",
      "    Worst case: 0.948385 seconds\n",
      "\n",
      "  Testing size 10000:\n",
      "    Best case: 0.000797 seconds\n",
      "    Average case: 1.922541 seconds\n",
      "    Worst case: 3.933995 seconds\n",
      "\n",
      "  Testing size 20000:\n",
      "    Best case: 0.001506 seconds\n",
      "    Average case: 7.977650 seconds\n",
      "    Worst case: 16.239343 seconds\n",
      "\n",
      "Insertion Sort benchmarking complete!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run systematic benchmarks for insertion sort\n",
    "print(\"Benchmarking Insertion Sort...\")\n",
    "for size in list_sizes:\n",
    "    print(f\"  Testing size {size}:\")\n",
    "    \n",
    "    # Best case (already sorted)\n",
    "    test_data = generate_test_data(size, 'best')\n",
    "    runtime = benchmark(insertion_sort, test_data)\n",
    "    benchmark_results['insertion_sort']['best']['sizes'].append(size)\n",
    "    benchmark_results['insertion_sort']['best']['times'].append(runtime)\n",
    "    print(f\"    Best case: {runtime:.6f} seconds\")\n",
    "    \n",
    "    # Average case (random)\n",
    "    test_data = generate_test_data(size, 'average')\n",
    "    runtime = benchmark(insertion_sort, test_data)\n",
    "    benchmark_results['insertion_sort']['average']['sizes'].append(size)\n",
    "    benchmark_results['insertion_sort']['average']['times'].append(runtime)\n",
    "    print(f\"    Average case: {runtime:.6f} seconds\")\n",
    "    \n",
    "    # Worst case (reverse sorted)\n",
    "    test_data = generate_test_data(size, 'worst')\n",
    "    runtime = benchmark(insertion_sort, test_data)\n",
    "    benchmark_results['insertion_sort']['worst']['sizes'].append(size)\n",
    "    benchmark_results['insertion_sort']['worst']['times'].append(runtime)\n",
    "    print(f\"    Worst case: {runtime:.6f} seconds\")\n",
    "    print()\n",
    "\n",
    "print(\"Insertion Sort benchmarking complete!\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking Selection Sort...\n",
      "  Testing size 1000:\n",
      "    Best case: 0.023999 seconds\n",
      "    Average case: 0.029696 seconds\n",
      "    Worst case: 0.018670 seconds\n",
      "\n",
      "  Testing size 2000:\n",
      "    Best case: 0.077549 seconds\n",
      "    Average case: 0.071596 seconds\n",
      "    Worst case: 0.072468 seconds\n",
      "\n",
      "  Testing size 5000:\n",
      "    Best case: 0.435669 seconds\n",
      "    Average case: 0.455633 seconds\n",
      "    Worst case: 0.522148 seconds\n",
      "\n",
      "  Testing size 10000:\n",
      "    Best case: 1.807789 seconds\n",
      "    Average case: 1.941558 seconds\n",
      "    Worst case: 1.854036 seconds\n",
      "\n",
      "  Testing size 20000:\n",
      "    Best case: 7.480379 seconds\n",
      "    Average case: 8.068434 seconds\n",
      "    Worst case: 8.223699 seconds\n",
      "\n",
      "Selection Sort benchmarking complete!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run systematic benchmarks for selection sort\n",
    "print(\"Benchmarking Selection Sort...\")\n",
    "for size in list_sizes:\n",
    "    print(f\"  Testing size {size}:\")\n",
    "    \n",
    "    # Best case (already sorted)\n",
    "    test_data = generate_test_data(size, 'best')\n",
    "    runtime = benchmark(selection_sort, test_data)\n",
    "    benchmark_results['selection_sort']['best']['sizes'].append(size)\n",
    "    benchmark_results['selection_sort']['best']['times'].append(runtime)\n",
    "    print(f\"    Best case: {runtime:.6f} seconds\")\n",
    "    \n",
    "    # Average case (random)\n",
    "    test_data = generate_test_data(size, 'average')\n",
    "    runtime = benchmark(selection_sort, test_data)\n",
    "    benchmark_results['selection_sort']['average']['sizes'].append(size)\n",
    "    benchmark_results['selection_sort']['average']['times'].append(runtime)\n",
    "    print(f\"    Average case: {runtime:.6f} seconds\")\n",
    "    \n",
    "    # Worst case (reverse sorted)\n",
    "    test_data = generate_test_data(size, 'worst')\n",
    "    runtime = benchmark(selection_sort, test_data)\n",
    "    benchmark_results['selection_sort']['worst']['sizes'].append(size)\n",
    "    benchmark_results['selection_sort']['worst']['times'].append(runtime)\n",
    "    print(f\"    Worst case: {runtime:.6f} seconds\")\n",
    "    print()\n",
    "\n",
    "print(\"Selection Sort benchmarking complete!\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BENCHMARK RESULTS SUMMARY ===\n",
      "\n",
      "Insertion Sort Results:\n",
      "  Best case:\n",
      "    Size  1000: 0.000107 seconds\n",
      "    Size  2000: 0.000157 seconds\n",
      "    Size  5000: 0.000918 seconds\n",
      "    Size 10000: 0.000797 seconds\n",
      "    Size 20000: 0.001506 seconds\n",
      "\n",
      "  Average case:\n",
      "    Size  1000: 0.020392 seconds\n",
      "    Size  2000: 0.064330 seconds\n",
      "    Size  5000: 0.490143 seconds\n",
      "    Size 10000: 1.922541 seconds\n",
      "    Size 20000: 7.977650 seconds\n",
      "\n",
      "  Worst case:\n",
      "    Size  1000: 0.037938 seconds\n",
      "    Size  2000: 0.164764 seconds\n",
      "    Size  5000: 0.948385 seconds\n",
      "    Size 10000: 3.933995 seconds\n",
      "    Size 20000: 16.239343 seconds\n",
      "\n",
      "\n",
      "Selection Sort Results:\n",
      "  Best case:\n",
      "    Size  1000: 0.023999 seconds\n",
      "    Size  2000: 0.077549 seconds\n",
      "    Size  5000: 0.435669 seconds\n",
      "    Size 10000: 1.807789 seconds\n",
      "    Size 20000: 7.480379 seconds\n",
      "\n",
      "  Average case:\n",
      "    Size  1000: 0.029696 seconds\n",
      "    Size  2000: 0.071596 seconds\n",
      "    Size  5000: 0.455633 seconds\n",
      "    Size 10000: 1.941558 seconds\n",
      "    Size 20000: 8.068434 seconds\n",
      "\n",
      "  Worst case:\n",
      "    Size  1000: 0.018670 seconds\n",
      "    Size  2000: 0.072468 seconds\n",
      "    Size  5000: 0.522148 seconds\n",
      "    Size 10000: 1.854036 seconds\n",
      "    Size 20000: 8.223699 seconds\n",
      "\n",
      "\n",
      "Benchmark data collection complete!\n",
      "Results stored in 'benchmark_results' dictionary for further analysis.\n"
     ]
    }
   ],
   "source": [
    "# Display summary of collected benchmark data\n",
    "print(\"=== BENCHMARK RESULTS SUMMARY ===\")\n",
    "print()\n",
    "\n",
    "for algorithm in ['insertion_sort', 'selection_sort']:\n",
    "    print(f\"{algorithm.replace('_', ' ').title()} Results:\")\n",
    "    for case in ['best', 'average', 'worst']:\n",
    "        sizes = benchmark_results[algorithm][case]['sizes']\n",
    "        times = benchmark_results[algorithm][case]['times']\n",
    "        print(f\"  {case.title()} case:\")\n",
    "        for i, (size, time_val) in enumerate(zip(sizes, times)):\n",
    "            print(f\"    Size {size:5d}: {time_val:.6f} seconds\")\n",
    "        print()\n",
    "    print()\n",
    "\n",
    "print(\"Benchmark data collection complete!\")\n",
    "print(\"Results stored in 'benchmark_results' dictionary for further analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Analysis for Complexity Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_complexity(sizes, runtimes):\n",
    "    \"\"\"Estimates time complexity using linear regression on log-transformed data.\"\"\"\n",
    "    # Convert to numpy arrays for easier manipulation\n",
    "    sizes = np.array(sizes)\n",
    "    runtimes = np.array(runtimes)\n",
    "    \n",
    "    # Log-transform the data\n",
    "    log_sizes = np.log(sizes)\n",
    "    log_runtimes = np.log(runtimes)\n",
    "    \n",
    "    # Perform linear regression on log-transformed data\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(log_sizes, log_runtimes)\n",
    "    \n",
    "    # Return slope coefficient (represents growth rate)\n",
    "    return {\n",
    "        'slope': slope,\n",
    "        'intercept': intercept,\n",
    "        'r_squared': r_value**2,\n",
    "        'p_value': p_value,\n",
    "        'std_error': std_err\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== COMPLEXITY ESTIMATION ANALYSIS ===\n",
      "\n",
      "Insertion Sort Complexity Analysis:\n",
      "  Best case:\n",
      "    Slope coefficient: 0.924\n",
      "    R-squared: 0.903\n",
      "    P-value: 0.013148\n",
      "\n",
      "  Average case:\n",
      "    Slope coefficient: 2.020\n",
      "    R-squared: 0.999\n",
      "    P-value: 0.000023\n",
      "\n",
      "  Worst case:\n",
      "    Slope coefficient: 2.010\n",
      "    R-squared: 1.000\n",
      "    P-value: 0.000001\n",
      "\n",
      "\n",
      "Selection Sort Complexity Analysis:\n",
      "  Best case:\n",
      "    Slope coefficient: 1.924\n",
      "    R-squared: 0.999\n",
      "    P-value: 0.000022\n",
      "\n",
      "  Average case:\n",
      "    Slope coefficient: 1.909\n",
      "    R-squared: 0.994\n",
      "    P-value: 0.000190\n",
      "\n",
      "  Worst case:\n",
      "    Slope coefficient: 2.030\n",
      "    R-squared: 1.000\n",
      "    P-value: 0.000005\n",
      "\n",
      "\n",
      "Complexity estimation complete!\n"
     ]
    }
   ],
   "source": [
    "# Calculate slope coefficients for all 6 algorithm-case combinations\n",
    "complexity_results = {}\n",
    "\n",
    "print(\"=== COMPLEXITY ESTIMATION ANALYSIS ===\")\n",
    "print()\n",
    "\n",
    "for algorithm in ['insertion_sort', 'selection_sort']:\n",
    "    complexity_results[algorithm] = {}\n",
    "    print(f\"{algorithm.replace('_', ' ').title()} Complexity Analysis:\")\n",
    "    \n",
    "    for case in ['best', 'average', 'worst']:\n",
    "        sizes = benchmark_results[algorithm][case]['sizes']\n",
    "        times = benchmark_results[algorithm][case]['times']\n",
    "        \n",
    "        # Estimate complexity using linear regression\n",
    "        analysis = estimate_complexity(sizes, times)\n",
    "        complexity_results[algorithm][case] = analysis\n",
    "        \n",
    "        print(f\"  {case.title()} case:\")\n",
    "        print(f\"    Slope coefficient: {analysis['slope']:.3f}\")\n",
    "        print(f\"    R-squared: {analysis['r_squared']:.3f}\")\n",
    "        print(f\"    P-value: {analysis['p_value']:.6f}\")\n",
    "        print()\n",
    "    \n",
    "    print()\n",
    "\n",
    "print(\"Complexity estimation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== THEORETICAL VS ESTIMATED COMPLEXITY COMPARISON ===\n",
      "\n",
      "Algorithm       Case     Theoretical          Estimated Slope R²       Match     \n",
      "================================================================================\n",
      "insertion sort  best     O(n) - slope ≈ 1.0   0.924           0.903    Good      \n",
      "insertion sort  average  O(n²) - slope ≈ 2.0  2.020           0.999    Good      \n",
      "insertion sort  worst    O(n²) - slope ≈ 2.0  2.010           1.000    Good      \n",
      "selection sort  best     O(n²) - slope ≈ 2.0  1.924           0.999    Good      \n",
      "selection sort  average  O(n²) - slope ≈ 2.0  1.909           0.994    Good      \n",
      "selection sort  worst    O(n²) - slope ≈ 2.0  2.030           1.000    Good      \n",
      "\n",
      "Legend:\n",
      "- Slope ≈ 1.0 indicates O(n) linear complexity\n",
      "- Slope ≈ 2.0 indicates O(n²) quadratic complexity\n",
      "- R² values closer to 1.0 indicate better fit to the regression model\n",
      "- 'Good' match means estimated slope is within 0.5 of theoretical expectation\n"
     ]
    }
   ],
   "source": [
    "# Create comparison table of theoretical vs estimated time complexities\n",
    "print(\"=== THEORETICAL VS ESTIMATED COMPLEXITY COMPARISON ===\")\n",
    "print()\n",
    "\n",
    "# Define theoretical complexities\n",
    "theoretical_complexities = {\n",
    "    'insertion_sort': {\n",
    "        'best': 'O(n) - slope ≈ 1.0',\n",
    "        'average': 'O(n²) - slope ≈ 2.0', \n",
    "        'worst': 'O(n²) - slope ≈ 2.0'\n",
    "    },\n",
    "    'selection_sort': {\n",
    "        'best': 'O(n²) - slope ≈ 2.0',\n",
    "        'average': 'O(n²) - slope ≈ 2.0',\n",
    "        'worst': 'O(n²) - slope ≈ 2.0'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create formatted comparison table\n",
    "print(f\"{'Algorithm':<15} {'Case':<8} {'Theoretical':<20} {'Estimated Slope':<15} {'R²':<8} {'Match':<10}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for algorithm in ['insertion_sort', 'selection_sort']:\n",
    "    for case in ['best', 'average', 'worst']:\n",
    "        theoretical = theoretical_complexities[algorithm][case]\n",
    "        estimated_slope = complexity_results[algorithm][case]['slope']\n",
    "        r_squared = complexity_results[algorithm][case]['r_squared']\n",
    "        \n",
    "        # Determine if estimated slope matches theoretical expectation\n",
    "        if 'slope ≈ 1.0' in theoretical:\n",
    "            expected_slope = 1.0\n",
    "        else:\n",
    "            expected_slope = 2.0\n",
    "        \n",
    "        # Consider a match if within 0.5 of expected slope\n",
    "        match = \"Good\" if abs(estimated_slope - expected_slope) <= 0.5 else \"Poor\"\n",
    "        \n",
    "        print(f\"{algorithm.replace('_', ' '):<15} {case:<8} {theoretical:<20} {estimated_slope:<15.3f} {r_squared:<8.3f} {match:<10}\")\n",
    "\n",
    "print()\n",
    "print(\"Legend:\")\n",
    "print(\"- Slope ≈ 1.0 indicates O(n) linear complexity\")\n",
    "print(\"- Slope ≈ 2.0 indicates O(n²) quadratic complexity\")\n",
    "print(\"- R² values closer to 1.0 indicate better fit to the regression model\")\n",
    "print(\"- 'Good' match means estimated slope is within 0.5 of theoretical expectation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up matplotlib for better plot appearance\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm-Focused Plots\n",
    "\n",
    "These plots show all three cases (best, average, worst) for each algorithm separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 1: Insertion Sort Performance Across All Cases\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Extract data for insertion sort\n",
    "sizes = benchmark_results['insertion_sort']['best']['sizes']\n",
    "best_times = benchmark_results['insertion_sort']['best']['times']\n",
    "avg_times = benchmark_results['insertion_sort']['average']['times']\n",
    "worst_times = benchmark_results['insertion_sort']['worst']['times']\n",
    "\n",
    "# Create the plot\n",
    "plt.plot(sizes, best_times, 'g-o', label='Best Case (Sorted)', linewidth=2, markersize=6)\n",
    "plt.plot(sizes, avg_times, 'b-s', label='Average Case (Random)', linewidth=2, markersize=6)\n",
    "plt.plot(sizes, worst_times, 'r-^', label='Worst Case (Reverse)', linewidth=2, markersize=6)\n",
    "\n",
    "plt.title('Insertion Sort Performance Comparison', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('List Size (n)', fontsize=12)\n",
    "plt.ylabel('Runtime (seconds)', fontsize=12)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 2: Selection Sort Performance Across All Cases\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Extract data for selection sort\n",
    "sizes = benchmark_results['selection_sort']['best']['sizes']\n",
    "best_times = benchmark_results['selection_sort']['best']['times']\n",
    "avg_times = benchmark_results['selection_sort']['average']['times']\n",
    "worst_times = benchmark_results['selection_sort']['worst']['times']\n",
    "\n",
    "# Create the plot\n",
    "plt.plot(sizes, best_times, 'g-o', label='Best Case (Sorted)', linewidth=2, markersize=6)\n",
    "plt.plot(sizes, avg_times, 'b-s', label='Average Case (Random)', linewidth=2, markersize=6)\n",
    "plt.plot(sizes, worst_times, 'r-^', label='Worst Case (Reverse)', linewidth=2, markersize=6)\n",
    "\n",
    "plt.title('Selection Sort Performance Comparison', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('List Size (n)', fontsize=12)\n",
    "plt.ylabel('Runtime (seconds)', fontsize=12)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case-Focused Plots\n",
    "\n",
    "These plots show both algorithms for each case separately, allowing direct comparison between algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 3: Best Case Comparison (Both Algorithms)\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Extract best case data for both algorithms\n",
    "sizes = benchmark_results['insertion_sort']['best']['sizes']\n",
    "insertion_best = benchmark_results['insertion_sort']['best']['times']\n",
    "selection_best = benchmark_results['selection_sort']['best']['times']\n",
    "\n",
    "# Create the plot\n",
    "plt.plot(sizes, insertion_best, 'b-o', label='Insertion Sort', linewidth=2, markersize=6)\n",
    "plt.plot(sizes, selection_best, 'r-s', label='Selection Sort', linewidth=2, markersize=6)\n",
    "\n",
    "plt.title('Best Case Performance: Insertion Sort vs Selection Sort', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('List Size (n)', fontsize=12)\n",
    "plt.ylabel('Runtime (seconds)', fontsize=12)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 4: Average Case Comparison (Both Algorithms)\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Extract average case data for both algorithms\n",
    "sizes = benchmark_results['insertion_sort']['average']['sizes']\n",
    "insertion_avg = benchmark_results['insertion_sort']['average']['times']\n",
    "selection_avg = benchmark_results['selection_sort']['average']['times']\n",
    "\n",
    "# Create the plot\n",
    "plt.plot(sizes, insertion_avg, 'b-o', label='Insertion Sort', linewidth=2, markersize=6)\n",
    "plt.plot(sizes, selection_avg, 'r-s', label='Selection Sort', linewidth=2, markersize=6)\n",
    "\n",
    "plt.title('Average Case Performance: Insertion Sort vs Selection Sort', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('List Size (n)', fontsize=12)\n",
    "plt.ylabel('Runtime (seconds)', fontsize=12)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 5: Worst Case Comparison (Both Algorithms)\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Extract worst case data for both algorithms\n",
    "sizes = benchmark_results['insertion_sort']['worst']['sizes']\n",
    "insertion_worst = benchmark_results['insertion_sort']['worst']['times']\n",
    "selection_worst = benchmark_results['selection_sort']['worst']['times']\n",
    "\n",
    "# Create the plot\n",
    "plt.plot(sizes, insertion_worst, 'b-o', label='Insertion Sort', linewidth=2, markersize=6)\n",
    "plt.plot(sizes, selection_worst, 'r-s', label='Selection Sort', linewidth=2, markersize=6)\n",
    "\n",
    "plt.title('Worst Case Performance: Insertion Sort vs Selection Sort', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('List Size (n)', fontsize=12)\n",
    "plt.ylabel('Runtime (seconds)', fontsize=12)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== PERFORMANCE VISUALIZATION SUMMARY ===\")\n",
    "print()\n",
    "print(\"Generated 5 performance plots:\")\n",
    "print(\"1. Insertion Sort Performance (all 3 cases)\")\n",
    "print(\"2. Selection Sort Performance (all 3 cases)\")\n",
    "print(\"3. Best Case Comparison (both algorithms)\")\n",
    "print(\"4. Average Case Comparison (both algorithms)\")\n",
    "print(\"5. Worst Case Comparison (both algorithms)\")\n",
    "print()\n",
    "print(\"Key Observations from Plots:\")\n",
    "print(\"- Insertion sort shows significant performance variation between cases\")\n",
    "print(\"- Selection sort shows consistent O(n²) behavior across all cases\")\n",
    "print(\"- Best case: Insertion sort dramatically outperforms selection sort\")\n",
    "print(\"- Average/Worst cases: Performance is more comparable between algorithms\")\n",
    "print(\"- All plots demonstrate clear quadratic growth patterns for most scenarios\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis and Reflection Questions\n",
    "\n",
    "### Theoretical vs Estimated Runtime Functions\n",
    "\n",
    "| Algorithm | Case | Theoretical | Estimated Slope | Match |\n",
    "|-----------|------|-------------|-----------------|-------|\n",
    "| Insertion Sort | Best | O(n) | 0.924 | Yes |\n",
    "| Insertion Sort | Average | O(n²) | 2.020 | Yes |\n",
    "| Insertion Sort | Worst | O(n²) | 2.010 | Yes |\n",
    "| Selection Sort | Best | O(n²) | 1.924 | Yes |\n",
    "| Selection Sort | Average | O(n²) | 1.909 | Yes |\n",
    "| Selection Sort | Worst | O(n²) | 2.030 | Yes |\n",
    "\n",
    "The estimates match theory well. All slopes are within 0.5 of expected values, and R² values above 0.9 indicate strong correlation.\n",
    "\n",
    "### Algorithm Performance Comparison\n",
    "\n",
    "**Insertion sort had better runtime in the best case.** For sorted data, insertion sort was dramatically faster (up to 5000x) because its inner loop rarely executes when data is already in order. The algorithm can skip most comparisons and shifts, achieving O(n) performance.\n",
    "\n",
    "In contrast, selection sort always performs the same number of comparisons regardless of input order, maintaining O(n²) behavior even for sorted data.\n",
    "\n",
    "### Practical Algorithm Choice\n",
    "\n",
    "**I would use insertion sort in practice** because:\n",
    "- It adapts to partially sorted data (common in real applications)\n",
    "- Best case performance is significantly better\n",
    "- Average case performance is comparable to selection sort\n",
    "- It's stable (maintains relative order of equal elements)\n",
    "\n",
    "However, for large datasets, neither algorithm is practical - modern O(n log n) algorithms like quicksort or mergesort should be used instead."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
